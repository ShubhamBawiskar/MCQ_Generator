import os
import json
import pandas as pd
import traceback
from dotenv import load_dotenv
from src.mcqgenerator.utils import read_file, get_table_data
from src.mcqgenerator.logger import logging
import ollama


# Langchain imports
from langchain_ollama.chat_models import ChatOllama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains import SequentialChain
from langchain.callbacks import get_openai_callback
import PyPDF2


# Model setup       
llm = ChatOllama(model="llama3.1",temperature=0.5)


# Template for generating MCQs using the provided text and guidelines
template = """
Text:{text}
You are an expert MCQ maker. Given the above text, it is your job to \
create a quiz of {number} multiple choice questions for {subject} students in {tone} tone. 
Make sure the questions are not repeated and check all the questions to conform to the text as well.
Make sure to format your response like RESPONSE_JSON below and use it as a guide. \
Ensure to make {number} MCQs.
### RESPONSE_JSON
{response_json} Reply should be in exact same format as given without adding extra words.
"""


# PromptTemplate for generating MCQs
quiz_generation_prompt = PromptTemplate(
    input_variables=["text", "number", "subject", "tone", "response_json"],
    template=template)

quiz_chain=LLMChain(llm=llm,prompt=quiz_generation_prompt,output_key="quiz", verbose=True)


# Template for evaluating the quiz generated by the model
template2 = """
You are a grammer expert in english and a writer. Given a Multiple Choice Quiz for {subject} students,\
you need to evaluate the complexity of the question and provide a complete analysis of the quiz. Use at max 50 words for complexity analysis. 
If the quiz is not at par with the cognitive and analytical abilities of the students,\
update the quiz questions which need to be changed, and adjust the tone to perfectly fit student abilities.
Quiz_MCQs:
{quiz}

Check from an expert English writer the above quiz:
"""


# PromptTemplate for evaluating and refining the quiz
quiz_evaluation_prompt = PromptTemplate(
    input_variables=["subject", "quiz"],
    template=template2)


# Chain for evaluating the quiz
review_chain = LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key="review", verbose=True)


# SequentialChain to handle quiz generation and evaluation
generate_evaluate_chain = SequentialChain(
    chains=[quiz_chain, review_chain],
    input_variables=["text", "number", "subject", "tone", "response_json"],
    output_variables=["quiz", "review"],
    verbose=True,
)


